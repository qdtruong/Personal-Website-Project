<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chatbot Instructions</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            background-color: #f4f4f9;
            color: #333;
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 900px;
            margin: 50px auto;
            padding: 20px;
            background-color: #fff;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
        }
        h2 {
            color: #333;
        }
        a {
            color: #007bff;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        .step {
            margin-bottom: 30px;
        }
        .step img {
            width: 800px;
            height: auto;
            display: block;
            margin: 10px auto;
        }
    </style>
</head>
<body>

<div class="container">
    <h2>Chatbot Instructions</h2>
    <p>How do you run a Large Language Model on a server that only has 1 gb of RAM and 20 gb of hard drive space? Basically...you set up a T4 run-time on Google Colab, tell it to use its GPU for computation, download/run an Ollama server, then expose that server to the internet using an ngrok tunnel. Then on this server, run a parallel instance of Ollama, feed commands to the Google Colab instance through the ngrok tunnel, and set up a Docker container running OpenWebUI to make for an easier user-experience. We will do all of that in the 2 steps below!</p>
    
    <div class="step">
        <h3>Step 1:</h3>
        <p>
            Click the following link: <a href="https://colab.research.google.com/drive/14o7Ck8dFI89SE-q831uK4aajHvWiLoIw" target="_blank">https://colab.research.google.com/drive/14o7Ck8dFI89SE-q831uK4aajHvWiLoIw</a>. This will open up a Google Colab page that will allow you to run code that I've written directly on Google's cloud servers. This code will install an LLM model on their servers, which will be accessible by this server.
        </p>
        <ul>
            <li>Run the script, either by pressing CTRL+F9, or by manually pressing play on each of the code blocks. It should take about 3 minutes total to run...about 1 minute for the first code block and 2 minutes for the 2nd code block.</li>
            <li>When everything is finished, you should see a terminal that looks like this:</li>
        </ul>
        <img src="/assets/images/terminal_example.png" alt="Terminal Example">
        <ul>
            <li>After seeing this, KEEP THE TAB OPEN. The Google Colab "play" button will keep spinning. This means that Colab is actively running the LLM model</li>
        </ul>
    </div>

    <div class="step">
        <h3>Step 2:</h3>
        <p>
            After ensuring that the Google Colab Script is actively running, click on this link. You should login in with:
        </p>
        <p><strong>USER:</strong> guest@qdtruong.com<br>
           <strong>PASSWORD:</strong> 123456</p>
        <p><a href="http://165.227.217.35:3000/?temporary-chat=true" target="_blank">http://165.227.217.35:3000/?temporary-chat=true</a></p>
        <p>
            You should now be able to converse with the LLM model you just created on Google Colab! REMEMBER TO DELETE YOUR CHATS AFTER YOU FINISH.
        </p>

    </div>
</div>

</body>
</html>
